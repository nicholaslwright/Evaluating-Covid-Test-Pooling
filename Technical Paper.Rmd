---
title: "Technical Paper"
author: "Andrew, Jorge, Karan, Lincoln, Nick"
date: "9/8/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Evaluating COVID Test Pooling

As a Walgreen's diagnostic-testing partner, we are determining how many tests we could possible conduct, given our limited testing materials. 

The first part of our analysis focuses on how we can maximize the number of people tested with a limited amount of tests. To do this we wanted to find the expected number of people tested per test kit.

The first step we took to setup our analysis was to build a function that would tell us how many tests it would take to screen every member of a given population size. We also took into consideration that the positive test rate within that population, which we gave the default of 9%, the current measurement at the time of this decision.

```{r}
# For a given population, what pool size will result in the least amount of tests required to test each person
expected_tests <- function(pop=10000, groupsize=4, positve_rate=0.09){

# How many groups can we compose from our population size?
numgroups = pop/groupsize

# This is the probability that a group will have at least one positive test, requiring a re-test
groupf<-(1-dbinom(0,groupsize, positve_rate))

# Based off of this failure rate, how many groups will need to retest?  
failedgroups<- numgroups*groupf

#For each group that requires a retest, the number of tests required to retest each person in that group is directly tied to the group size (since we will test each member)
newtests<- groupsize*failedgroups

# The total number of tests required to evaluate a given number of people
if(groupsize>1){
totaltests<-newtests+numgroups
}
else{
  totaltests=pop
}

return(totaltests)
}
```


## Evaluating different cases

Once we had the function established, we wanted to understand how effective each pool size would be at maximizing the number of people we could screen with a given amount of tests. 

```{r}

# Depending on the accuracy you'd like to achieve set the population size, greater population equals greater accuracy
pop <- 10000

# Initializing the matrix where we will collect the expected # of tests required to test everyone in the given population size, depending on two variables: the pool size (rows), and the positive test rate of the population (columns)
data <- matrix(nrow=25, ncol=100)

# This loop populates the above matrix with the expected value for number of tests
for(positive_rate in 1:100){
  for(group_size in 1:25){
    data[group_size, positive_rate] = expected_tests(pop,group_size,positive_rate/100)
  }
}
```


After we arrived at this matrix, with rows reflecting the possible variation of pool size, and columns representing the possible states of the proportion of the population testing positive, we wanted to go a step further and understand the optimal pool size for each possible proportion. This would, in effect, let us know how sensitive pool size is to the proportion of the population testing positive.

```{r}
# To abstract the results a bit further, I'd like to see, on average, how many people we can evaluate per test. This way we are achieving the maximum results with each test
tests_per_capita <- pop/data


# I also want to see how different positive test rates in the population affect our optimal pool size
optimal <- 1:100

# For each positive test rate of the population (1%, 2%, 3%, ...) this will tell us what the optimal pool size is for that context
for(positive_rate in 1:100){
  optimal[positive_rate] <- which(tests_per_capita[,positive_rate]==max(tests_per_capita[,positive_rate]))
}

```

With this vector we can see how the optimal pool size changes in response to the population's Positivity Rate. (See Plot Below)

```{r}
plot(optimal)
```

